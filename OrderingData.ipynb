{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Overview](overview.ipynb)\n",
    "\n",
    "# Data Structures:  Keeping Data Organized\n",
    "#### by Kirby Urner\n",
    "\n",
    "The verb \"keep\" is interesting, in that if you've studied the anatomy of castles, you'll know the central building, where the crown, jewels and sceptre are kept, is called the Keep.\n",
    "\n",
    "Keeping Data, might be a term of art, although it currently isn't.\n",
    "\n",
    "## Anatomy of a Website\n",
    "\n",
    "Sometimes, in fairy tales, [a dragon](https://tvtropes.org/pmwiki/pmwiki.php/Main/DragonHoard) keeps guard over [the hoard of treasure](https://youtu.be/MH49zLTeQsE).\n",
    "\n",
    "<a data-flickr-embed=\"true\"  href=\"https://www.flickr.com/photos/kirbyurner/31900281608/in/dateposted-public/\" title=\"youtube_django\"><img src=\"https://farm5.staticflickr.com/4883/31900281608_1851b05ce2.jpg\" width=\"500\" height=\"360\" alt=\"youtube_django\"></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "<br/>\n",
    "As you may have heard me explain in class, the smokey snake emerging from a genie lamp in this crudely drawn picture (below) represents Python.  \n",
    "\n",
    "Authentication and data storage are likely two of its perviews (where storage is full CRUD:  Create Retrieve Update Delete.  \n",
    "\n",
    "The Python DBI is well suited to chatting with SQL engines.  NoSQL engines (e.g. Neo4j) provide drivers as well.\n",
    "\n",
    "That's all server-side behind the scenes.  The client browser gets the JavaScript puppet show, meaning the client side stays responsive and animated in an asynchronous process teathered to the server through what we call AJAX, or more generally, backdoor exhanges of JSON, XML, HTTP and so on.\n",
    "\n",
    "A classic web framework will provide a Model, View, Controller architecture, wherein the views show changes made in the model, in reaction to the controller doing stuff.\n",
    "\n",
    "In [a tinyflask website](https://github.com/4dsolutions/tiny_flask), used in the course, we take a look at a simple anatomy, featuring Flask atop SQLite.\n",
    "\n",
    "<a data-flickr-embed=\"true\"  href=\"https://www.flickr.com/photos/kirbyurner/9583829639/in/photolist-27Qk7d8-XdQNYJ-VcSFee-oy7oP7-fATCE4-9oKHcV-9mV46z-8JMt7W-7uuc4g-7iy6jr-76Xswj-76XsFm-6WcupH-6QZD8h-6QVzKx-6y72Mo-6xXBCb-6xNgjY-5JVBmV-5z7Qey-5f9rAd-5f54mc\" title=\"Browser as Theater\"><img src=\"https://farm8.staticflickr.com/7301/9583829639_8199c0b3c6.jpg\" width=\"500\" height=\"375\" alt=\"Browser as Theater\"></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "We introduce data structures through the lense of a website, because that's very typically how we intereact with data stores.\n",
    "\n",
    "The Python language is designed from the ground up for working with data.\n",
    "\n",
    "## Built In Data Structures\n",
    "\n",
    "Probably the first data structure you encounter in Python will be the list, although technically the string is a data structure (of letters, or str type objects).  \n",
    "\n",
    "Then come the tuple (like an immutable list) and the dict (not even a sequence).  \n",
    "\n",
    "All of these types use square brackets (`[]`) to retrieve contents, and matching up with square brackets are Python special names ```__getitem__``` and ```__setitem__```.\n",
    "\n",
    "As Alex Martelli points out in one of his lectures, special names are deliberately \"ugly\" because we want to steer coders towards using square brackets when feasible.  Sometimes, though, we have to get behind the scenes, under the surface, into the guts of Python, and that's where special names (or ```__ribs___```) as I call them, come into focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thing of <class 'tuple'> (-1, 2, 3, 'a', '🐙', '🐳', '🐯') \n",
      "\n",
      "thing of <class 'list'> ['a', '🐙', '🐳', '🐯'] \n",
      "\n",
      "thing of <class 'str'> a🐙🐳🐯 \n",
      "\n",
      "thing of <class 'dict'> {-1: 2, 2: 3, 3: 'a', 'a': '🐙', '🐙': '🐳', '🐳': '🐯'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### BUILT-IN DATA STRUCTURES\n",
    "\n",
    "# TUPLE\n",
    "the_tuple = (-1, 2, 3, 'a', '🐙', '🐳', '🐯')  # <-- emoji are Unicode\n",
    "\n",
    "# LIST\n",
    "the_list = list(the_tuple[3:])  # <-- slice off the first 3, grab the rest\n",
    "\n",
    "# STRING\n",
    "the_str = \"\".join(the_list)  # <-- glue list elements together with empty string\n",
    "\n",
    "# DICT\n",
    "the_dict = dict(zip(the_tuple[:-1], the_tuple[1:])) # <-- pair each element with next in line\n",
    "\n",
    "for thing in the_tuple, the_list, the_str, the_dict:\n",
    "    print(\"thing of\", type(thing), thing, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is pretty fancy, not \"beginner\" necessarily.  We start using slice notation, with a colon in brackets, right off the bat.  You'll see slices again when we get to Numpy.\n",
    "\n",
    "Notice how square brackets keep cropping up, and with the same meaning:  we're using them to retreive (the R in CRUD).  Square brackets are also used to update.\n",
    "\n",
    "In the case of sequence types (string, list, range, tuple...) we speak of \"indexes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🐙'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of mappings (dicts), we speak of \"keys\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🐳'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_dict['🐙']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below demonstrates how ```__getitem__``` is the verb behind use of square brackets.  ```obj[n]``` translates to ```obj.__getitem__(n)``` in Python's mind (interpretation).  Likewise ```obj[n] = x``` is the same as saying ```obj.__setitem__(n) = x```.\n",
    "\n",
    "The cell below emphasizes the uniformity of the API, meaning all these data structures use ```__getitem__``` to implement square bracket syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐯, 🐯\n",
      "🐯, 🐯\n",
      "🐯, 🐯\n",
      "2, 2\n"
     ]
    }
   ],
   "source": [
    "for thing in the_tuple, the_list, the_str, the_dict:\n",
    "    print(thing.__getitem__(-1), end=\", \")  # all four use __getitem__, -1 means \"last thing\"\n",
    "    print(thing[-1]) # ... except in the case of the_dict, where it's a key, not an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below returns pseudo-random strings of 1s and 0s, of the requested length.  \n",
    "\n",
    "We're going to use such strings to build a pseudo data set suitable for feeding to a Machine Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000001\n",
      "101100\n",
      "001011\n",
      "000000\n",
      "001111\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "def get_random_row(n):\n",
    "    return \"\".join([str(choice([0,1])) for _ in range(n)])\n",
    "\n",
    "for _ in range(5):\n",
    "    sample = get_random_row(6)\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy's ndarray\n",
    "\n",
    "The next data structure we often encounter in a Pythonic project, if doing any kind of number crunching, is the star of numpy:  the n-dimensional array, or ndarray.\n",
    "\n",
    "The cells below show ndarrays getting used in a typical fashion:  we feed them to a Machine Learning algorithm, a classifier.\n",
    "\n",
    "Our example is deliberately meaningless, meaning we'll pick an arbitrary pattern of 1s and 0s to match with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_data(n):\n",
    "    \"\"\"\n",
    "    Only the three patterns singled out below \n",
    "    Patterns: 1,2,3\n",
    "    \"\"\"\n",
    "    X = np.empty(shape=(n,6), dtype=int)\n",
    "    Y = np.empty(shape=(n,),  dtype=int)\n",
    "    for row in range(n):\n",
    "        features = get_random_row(6)\n",
    "        y = 0  # unless a pattern matches\n",
    "        \n",
    "        # Patterns\n",
    "        if features == \"100001\": # 1\n",
    "            y = 1\n",
    "        if features == \"010010\": # 2\n",
    "            y = 1\n",
    "        if features == \"001100\": # 3\n",
    "            y = 1\n",
    "            \n",
    "        Y[row]    = y  # list of \"right answers\"\n",
    "        # fancy list comprehension to convert strs to ints\n",
    "        X[row, :] = list(map(lambda x: int(x), features))\n",
    "    return X, Y\n",
    "    \n",
    "train_samples, train_answers = make_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1]\n",
      " [0 1 1 0 0 0]\n",
      " [0 0 0 0 1 1]\n",
      " [1 1 0 1 1 0]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_samples[0:5]); print(train_answers[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, lets print the first 10 rows for which the corresponding ```train_answer``` is 1.  This is to be sure we understand which patterns match with 1, and to confirm the algorithm is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 1]\n",
      "[0 0 1 1 0 0]\n",
      "[0 1 0 0 1 0]\n",
      "[0 0 1 1 0 0]\n",
      "[0 0 1 1 0 0]\n",
      "[0 0 1 1 0 0]\n",
      "[0 0 1 1 0 0]\n",
      "[0 1 0 0 1 0]\n",
      "[1 0 0 0 0 1]\n",
      "[0 1 0 0 1 0]\n",
      "[1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "lines = 0\n",
    "for idx, sample in enumerate(train_samples):\n",
    "    if train_answers[idx]:\n",
    "        print(sample)\n",
    "        lines += 1\n",
    "    if lines > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from Data\n",
    "\n",
    "Now we're ready to take one of the Machine Learning algorithms down from the shelf.  Lets go with KNN and fit the training data so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(train_samples, train_answers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = [0., 1., 0., 0., 1., 0.]\n",
    "data = [0, 0, 1, 1, 0, 0]\n",
    "test = np.array(data, dtype=float)\n",
    "test = test.reshape((1,6))\n",
    "neigh.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is about validating that our model is sea-worthy.  We make new data, of course following the same pattern, and see if our model ```neigh``` is up to the job of recognizing for which patterns to predict a 1, all other times 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples, test_answers = make_data(50)\n",
    "neigh.score(test_samples, test_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "Little Data is very important i.e. \"big\" does not mean \"more meaningful\" it just means \"more data\" often a lot more.  When the data takes up a lot of space, then how it's managed, and how it's run through Machine Learning algorithms, becomes a topic in itself. Additional tools enter the picture, such as Apache Spark and Hadoop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
